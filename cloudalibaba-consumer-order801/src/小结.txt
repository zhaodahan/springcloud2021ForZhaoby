这里是准备使用springCloudAlibaba

 使用nacos 作为 服务注册中心
先自己摸索一便

cloudalibaba-consumer-order801
1:消费者首先需要注册到nacos中去
2: 思考如何调用生产者
方案使用fegin
先把controller 整出来

先分别测试调用
验证生产者
http://localhost:8848/nacos

http://localhost:9001//payment/nacos/1
验证消费者调用生产者
http://localhost:8001//consumer/payment/get/2

成功了

再看着脑图弄一遍
cloud-consumerAlibaba-order802

===========================================脑图步骤总结：===================
1：pom 引入
主要两个；
spring cloud alibaba 整体jar
<!--spring cloud alibaba 2.1.0.RELEASE-->
<dependency>
  <groupId>com.alibaba.cloud</groupId>
  <artifactId>spring-cloud-alibaba-dependencies</artifactId>
  <version>2.1.0.RELEASE</version>
  <type>pom</type>
  <scope>import</scope>
</dependency>

引入nacos  用于nacos 服务注册发现
  <dependency>
        <groupId>com.alibaba.cloud</groupId>
        <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
    </dependency>

还有就是监控jar
  <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-actuator</artifactId>
    </dependency>

2：yml 配置文件配置
除了配置基本的端口和服务名
这个yml 主要配置的是nacos的注册地址
spring:
  application:
    name: XXXX
  cloud:
    nacos:
      discovery:
        server-addr: localhost:8848 #配置Nacos地址

还有就是springboot 的监控管理需要开启
(exposure 暴露所有健康检查的接口)
management:
  endpoints:
    web:
      exposure:
        include: '*'

3：主启动类上除了正常的boot 应用注解，还要加上@EnableDiscoveryClient 标识该服务启用了服务发现功能
springboot 在启动的时候才会去加载相应的配置
@SpringBootApplication
@EnableDiscoveryClient

4： 实现业务类

5： 服务消费者上面步骤相同， 消费者也是一个微服务 需要注册到nacos 中去
不过，如果要使用Fegin 去调用生产者服务
pom 中需要引入 openfeign
  <dependency>
            <groupId>org.springframework.cloud</groupId>
            <artifactId>spring-cloud-starter-openfeign</artifactId>
  </dependency>

启动类中需要加上@EnableFeignClients 注解，表明这是一个Fegin 调用客户端
@SpringBootApplication
@EnableDiscoveryClient
@EnableFeignClients

业务调用服务处需要 明确注解声明
@Component
@FeignClient(value = "nacos-payment-provider")
public interface PaymentFeignService {

===========================================脑图步骤总结：end===================
最后再看视频
cloud-consumerAlibaba-order803
===========================================视屏总结：===================
1： nacos 自带负载均衡
因为spring-cloud-starter-alibaba-nacos-discovery  中阿里集成了ribbon
ribbon负责负载均衡


===========================================视屏总结：end=================


下一步摸索 nacos 如何作为配置服务配置中

1： 自己摸索，思考原理
========================================================================================================================
作为服务配置中心
1：远程肯定需要存储我们的配置文件
本地服务是肯定是作为客户端， nacos 作为服务端。 只能将配置信息存储在nacos 中
如何配置，参考官网
https://spring-cloud-alibaba-group.github.io/github-pages/greenwich/spring-cloud-alibaba.html#_how_to_introduce_nacos_config_for_configuration
2：文件需要去远程拉去配置信息

肯定存在3步

1： 引入pom包
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
</dependency>

(注意：nacos-config 要放在 nacos-discovery 前， 否则IDEA 会报错)

2；nacos  管理台中配置配置文件

nacos 配置管理台上建立文件Data ID 使用"服务名.yml" 分组使用默认分组


3： 配置文件配置引入配置文件
需要在 bootstrap.yml 中配置 Nacos 服务器的地址和该服务对应使用的哪个文件
需要新建一个bootstrap.yml 文件
spring:
  application:
    name: nacos-payment-provider
  cloud:
    nacos:
      config:
        server-addr:127.0.0.1:8848
这里默认指定的配置为 GROUP为DEFAULT_GROUP  DataId为`spring.application.name` 配置结合文件扩展名（配置格式默认使用properties）


4： 业务代码使用配置文件中的信息

5: 测试结果： 发现无法启动生产者服务
可能原因， 在上面没有指定具体使用哪个配置文件
还有一个可能，nacos 中配置文件格式没有yml ,只有yaml 。
我们强加的配置文件后缀是yml

6: 修改配置文件后缀
修改后还是报错
Caused by: com.alibaba.nacos.api.exception.NacosException: endpoint is blank
查询发现是：源代码为判断了 serverAddrStr为空，然后endpoint 为空 所以抛出了异常。
检查了nacos config 配置的bootstarp.yml 发现配置文件配置的有问题
(这里还是需要注意)

bootstarp.yml 内容最终修改为
spring:
  application:
    name: nacos-payment-provider
  cloud:
    nacos:
      config:
        server-addr: 127.0.0.1:8848
        file-extension: yaml

修改后发现启动成功，测试读取配置文件中的内容也是成功的

================================================================================================
2： 对照脑图自己操作
总结脑图步骤
1； pom 中引入nacos-config. 启用nacos 作为配置中心的功能

2：配置yml  bootstarp.yml (本地) + application.yml (远程/本地)
为什么需要两个配置文件
因为 nacos 和是springcloud 一样在项目初始化启动的时候需要先从远程拉取配置才能正常启动。
bootstarp.yml中配置的就是从哪个远程拉取哪个配置
springboot中配置文件的加载顺序 bootstarp.yml > application.yml
bootstarp.yml 也可以比作另外一种形式的 application.yml

bootstrap.yml 中定位nacos 中配置文件的规则
dataId 的默认格式是： ${prefix}-${spring.profiles.active}.${file-extension}
prefix 默认为 spring.application.name 的值，也可以通过配置项 spring.cloud.nacos.config.prefix来配置。
spring.profiles.active 即为当前环境对应的 profile
file-exetension 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置。
目前只支持 properties 和 yaml 类型。


3: nacos 中配置自动刷新
官网文档说支持自动刷新，不过是在版本 cloud greenwich.  这里是用得是Hoxton 。  albaba 版本2.1.2 。

这里版本比较低，需要自己加注解去刷新配置
使用方法： 在业务controller 中加上@RefreshScope 注解来实现配置自动更新
//https://nacos.io/zh-cn/docs/quick-start-spring-cloud.html
（经测试，确实能够刷新配置）




================================================================================================
3： 观看视屏总结

1： 以前我们完成配置的自动刷新，需要结合springcloud config + bus . 现在我们将其解耦，直接全部整合进nacos. 使用nacos 来独立完成
    我们将配置文件的远程从github 迁移到nacos 上了

2；  bootstarp.yml 共性
     application.yml 个性   常在 application.yml 中指定开发环境  spring.profiles.active 用以切换不同环境的配置文件

3：@RefreshScope 注解的作用是，当远程配置发生变化，通知到客户端服务的时候，可以刷新服务应用去重新读取新的配置的值

================================================================================================

nacos 集群 和集群化配置 (重要)

生产环境上必定是要nacos 集群的。

如何配置nacos集群 (防止单点故障)？
https://nacos.io/zh-cn/docs/cluster-mode-quick-start.html （运维集群部署）

生产环境配置集群需要准备：

1：生产部署环境Linux。
2：节点至少3个，主备模式 构建集群  (集群通过Nginx来进行转发)
3：外置数据源 。生产使用建议至少主备模式，或者采用高可用数据库 (mysql)。
(因为 nacos 中存储着 配置文件信息。 这些重要的配置文件信息为了安全可见 ，需要进行持久化)

为什么需要外置数据源？
因为nacos 默认使用的内嵌入式数据库来存储的数据。 即每个nacos 节点都内置一个数据库来存储数据
使用配置集群多个节点， 数据存储的一致性就存在问题。
nacos为了解决这个问题，采用了集中式存储的方式来支持集群化部署。 目前只支持mysql.

我们知道配置集群+配置文件的持久化都需要使用外置数据源， 然而nacos 已经内置了一个数据库，我们应该如何切换数据源？
如何从默认的derby 切换到MySQL?

按照官网文档需要两步
1：初始化mysql 数据库。 (配置数据库和建存储数据的表)
出事化sql脚本：  nacos-server-1.1.4\nacos\conf\nacos-mysql.sql

2：修改nacos启动的 application.properties 配置，配置上mysql的数据源

======================配置文件中搜索关键词 datasource.platform=====================
spring.datasource.platform=mysql
db.num=1
db.url.0=jdbc:mysql://localhost:3306/nacos_config?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true
db.user=root
db.password=123456
===========================================
完成上述两步重启后，就切换了数据源

====================================================如何开启集群：==================================================
官网文档教程： 1个nginx+3个nacos注册中心+1个mysql

1： 需要一个Nginx vip (virtual IP ) 所有请求都是经过Nginx 转发到nacos . （Nginx 也可配置为集群）
外部微服务只需要注册到一个ip 即可。
测试通过nginx访问nacos：https://写你自己虚拟机的ip:1111/nacos/#/login

2： 3个nacos 节点， 避免单点故障
核心 修改每个节点  nacos/conf/cluster.conf 配置文件
每行配置成ip:port。（配置3个或3个以上节点） 配置集群中节点的地址 (告诉nacos 我们这些节点属于一个集群)
如：
# ip:port
200.8.9.16:8848
200.8.9.17:8848
200.8.9.18:8848

(同一台机器 ，可以通过修改启动脚本，根据传入的不同端口，启动不同的nacos 实例 )
3： mysql外置数据源，统一数据源存储， 避免各自为战

=================================================================================================================

sentinel 是什么？ 流量控制，服务降级
核心作用是用于实现熔断和限流。 监控和保护我们的微服务。
分布式系统的流量防卫兵。 =》hystrix 的阿里版
他与hystrix 不同之处(优势)在于 hystrix 只能针对一个具体的微服务做服务降级和熔断。 (高并发下可以将某个服务降级关闭)
而sentinel可以针对某个请求配置对应的服务限流和降级规则。

理论原理：
1:Sentinel的核心是以“流量”为切入点。在流量控制、断路、负载保护等多个领域开展工作，保障服务可靠性
就是以流量为基础，进而通过控制流量，来进行断路和控制访问

2：sentinel的控制台由两部分组成。
类似hystrix ,
一个后台的微服务 核心库(后台的监控程序，maven引入jar包)，
一个dashboard 监控台8080端口 (前台的展示界面，基于springboot开发，打包后可运行，无需额外的Tomcat容器)。




sentinel 下载
https://github.com/alibaba/Sentinel/releases
这里下载的是个dashbord  jar 包,sentinel 的监控仪表盘

怎么使用？
https://spring-cloud-alibaba-group.github.io/github-pages/greenwich/spring-cloud-alibaba.html#_spring_cloud_alibaba_sentinel

sentinel dashboard  运行：
他以来于8080 端口，需要保证端口不被占用
他的本质是个jar包（jar 包如何运行？ Java -jar）
所以控制台的访问地址是：http://localhost:8080/#/login

核心库监控程序 安装进微服务

1: 引入pom
     <dependency>
            <groupId> com.alibaba.cloud </groupId>
            <artifactId> spring-cloud-starter-alibaba-sentinel </artifactId>
        </dependency>

2: 配置yml
spring:
  cloud:
    sentinel:
      transport:
        port: 8719
        dashboard: localhost:8080

spring.cloud.sentinel.transport.port
指定的端口号将在应用程序的相应服务器上启动一个 HTTP Server，该服务器将与 Sentinel 仪表板进行 交互 (中间人)。
例如，如果在 Sentinel 仪表板中添加了速率限制规则，则规则数据将被推送到 HTTP Server 并由 HTTP Server 接收，HTTP Server 进而将规则注册到 Sentinel。

3: 业务代码中使用@SentinelResource("XX")
  @GetMapping(value = "/payment/nacos/{id}")
    @SentinelResource("/payment")
    public String getPayment(@PathVariable("id") Integer id){}

4: 微服务应用需要进行访问过才能在dashboard 中监控到 (sentinel 使用的是懒加载机制，访问一次后才会触发)。

5： 问题： 感觉@SentinelResource("/payment") 这个资源配置似乎了没起作用？

==========================================================================
sentinel 流控规则
基础概念：
资源：唯一标识,默认为请求路径
流量阈值类型：
 QPS：每秒请求数量
 线程数： 调用该api 的线程数（同时有多个线程调用该api）
两种阈值的区别：
  QPS 是在进入资源的外部进行拦截，御敌于国门之外。
  线程数阈值： 就是在资源内干活的只有n个， 里面的活没有干完，进去的就超过了限制，活干不完了。直接失败或者。。。


流控模式
 直接： 对该资源api 访问达到一定的阈值后进行限制
 关联： 当我们设置的 关联的资源达到阈值时，限制我们指定的资源 (监控的是我们关联的资源)
       与A关联的B 去惹事，A 受到牵连，限制A. ====>B惹事，A 挂了
       作用： 比如 支付接口达到阈值后，限制下单接口。防止连坐效应。 我这边压力大了， 你那边就不要过来了
 链路： 从指定入口资源进入的链路上的流量达到阈值，就限制我们指定的资源


流控效果： 上面流控模式限制的效果
 快速失败： 直接失败，抛出异常
 warm up 预热 慢慢涨上来 ： 通过预热达到指定的QPS.  在指定时间内QPS匀速上升，从阈值/codeFactor(冷加载因子)
          不要一上来就大量的请求，可能限制一些资源还没有加载起来，直接就将系统压垮，允许系统慢慢启动起来。
          应用场景，秒杀开启的瞬间，会有大量请求过来，可能会把系统打死。这时预热就可以保护系统
 排队等待： 必须设置QPS .让超过阈值的请求排队，匀速通过（控制请求通过的间隔时间，漏桶算法）
          应用场景，系统每秒只能处理n个请求，多余过来的请求就排队。超时就重试。优先确保系统的稳定
          即请求过来了既没有将系统冲跨，又没有拒绝(双赢)。

=========================================================================
sentinel 降级 类似于Hystrix的服务熔断

sentinel 熔断降级会在调用链路中某个资源出现不稳定状态是(调用超时，异常比例升高)，
对资源的调用进行限制，让请求快速失败，避免影响其他资源调用，而导致级联错误。

当资源被降级后，在接下来的降级时间窗口内，对该资源的调用都是自动熔断。 (默认是快速失败，抛出异常)
sentinel 的熔断与hystrix的熔断不同是，sentinel的熔断“不存在半开” 状态，要么是熔断，要么是正常调用
半开：就是系统会自动的去检测是否还存在异常请求调用，如果不存在了，就关闭断路器，恢复使用
sentinel的熔断恢复是：是在熔断窗口时间内，一直处于自动熔断，而过了时间窗口自动恢复，如果接下来的请求调用仍然处于不稳定状态，继续进入熔断。

三种降级策略：https://github.com/alibaba/Sentinel/wiki/%E7%86%94%E6%96%AD%E9%99%8D%E7%BA%A7
慢调用比例 ：顾明思意，就是在单位时间内统计慢调用的比例，超过阈值就进行熔断。
          什么样的被定义为慢调用：需要设置最大 RT (最大相应时间)，超过就是慢调用。
          慢调用比例：就是在单位请求时间内，慢调用所占的比例。
          统计 秒级

异常比例 (统计 秒级)

异常数 (统计 分钟级)，如果时间窗口<60 秒 (统计的单位时间)，在系统还未恢复，就中断了熔断，则下一次仍然可能进入熔断
      所以这里劲量将时间窗口时间设置时长超过60 秒

这里的时间窗口就是用来留给系统恢复的时间
熔断的目的是为了保护系统安全

=========================================================================
热点key限流 (他也是限流的一种)

什么是热点：即访问频率高的数据。 有时候我们需要统计访问最多的topK 数据，并对其限流
我们可以根据请求带有的参数进行限流。

热点key限流： 就是针对热点参数限流。他会统计传入参数中的热点参数，并根据配置的限流阈值与模式，对包含热点参数的资源调用进行限流。
举例： http://localhost:9001//payment/nacos?p=key  针对请求中的参数P 做限流，只要参数中带有P 就会触发限流规则
      资源名是请求， 参数索引是对应的请求中的参数 (目前似乎只对gei 请求有效。 post 请求可能无法抓取到具体参数)

参数例外项： 举例: 我们针对P 的QPS 限流是10 ，但是我们可以配置当P=5 时限流规则为QPS=100;

=========================================================================
系统自适应限流
从整体维度对应用入口流量进行控制。 (之前的sentinel限流规则只能针对单个请求，但是每个都配置一个限流规则，十分麻烦，现在就有了一个整体的维度来补充控制)
之前是微服务内部 具体每个请求限流，现在是微服务外部一个整体来进行配置限流。
结合应用的 Load(自适应)、CPU 使用率、总体平均 RT、入口 QPS 和并发线程数等几个维度的监控指标，通过自适应的流控策略，
让系统的入口流量和系统的负载达到一个平衡，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定性。

（正因为是整体维度，是大门级别，配置小了就很危险，实用性有待考量）

=========================================================================
@SentinelResource 注解 (和@HystrixConmand 类似)
问题1: 这个注解有什么用？
用来将服务中的接口定义为sentinel的监控资源。
    @GetMapping(value = "/payment/nacos/{id}")
    @SentinelResource("/payment",blockHandler  = "handleException")
    public String getPayment(@PathVariable("id") Integer id)
在sentinel的控制台簇点链路中我们可以看到两个资源 /payment/nacos/{id} 和/payment
他们都可以作为 针对该接口的资源来配置限流规则
那他们的不同之处在哪？
1: 如果使用sentinel自动根据路径生成的资源来配置限流规则，我们没法使用自定的限流异常。
   而 @SentinelResource("/payment",blockHandler  = "handleException") handleException 就是我们自定义的限流返回
   其中blockHandlerClass 属性还可以指定 自定义异常的方法所在的类

2： BlockException 只能补货限流，如果是其他运行时异常，需要fallback 属性  (两者各司其职)，fallback 需要多声明Throwable 异常
当限流和异常同时都满足时， 具体时限流还是业务异常？
限流的优先级更高。
为什么？因为作为系统来说给用户展示友好信息肯定优于直接展示个报错。

3：假如某些异常不需要兜底方法fallback ,可以使用属性exceptionsToIgnore 用于指定异常不走兜底方法
exceptionsToIgnore = ArithmeticException.class
接显示错误信息给前台页面。

问题2； 如何使用
  @GetMapping(value = "/payment/nacos/{id}")
    @SentinelResource(value = "/payment",blockHandler  = "handleException")
    public String getPayment(@PathVariable("id") Integer id)
    {
        return "nacos registry, serverPort: "+ serverPort+"\t id"+id+"\t yml file content"+testFileContent;
    }


    public String handleException(BlockException exception)
    {
        return "自定义限流handleException";
    }

3： 使用时出现问题： 限流并没有触发我们自定义的限流规则，而是抛出了异常Caused by: com.alibaba.csp.sentinel.slots.block.flow.FlowException'

是什么原因？ 如何解决
经过加日志验证是，压根没有进入我们自定义的异常方法。说明程序压根没有找到该方法
错在哪了？对照写法，猜测是自定义异常方法写的不对，应该将参数都带上
public String handleException(@PathVariable("id") Integer id,BlockException exception)

修改之后发现自定义异常出现了，就是这个问题，如果方法中带有参数。 自定方法参数要一致，且要带上BlockException exception

正确用法：
@GetMapping(value = "/payment/nacos/{id}")
    @SentinelResource(value = "payment",blockHandler  = "handleException")
    public String getPayment(@PathVariable("id") Integer id)
    {
        return "nacos registry, serverPort: "+ serverPort+"\t id"+id+"\t yml file content"+testFileContent;
    }


    public String handleException(@PathVariable("id") Integer id,BlockException exception)
    {
        System.out.println("进入了自定义的限流异常方法");
        return "自定义限流handleException";
    }



===========================================================================
服务熔断
sentinel整合ribbon+openFeign+fallback

sentinel使用openFeign 一定要在配置中开启sentinel对feign 的支持
#对Feign的支持
feign:
  sentinel:
    enabled: true


使用Fegin ，启动类上不要忘记加 @EnableFeginClients 注解

服务降级，就是当检测到目标服务不可用时，不用一直消耗资源等待，可以直接返回预期异常，结束调用，避免服务熔断.
生产测熔断，消费测自动降级

===========================================================================
sentinel 的持久化 (流控规则持久化)
为什么需要持久化？
因为每次服务重启后，sentinel中配置的流控规则就会消失，需要重新配置。‘
如果是生产环境? 工作量和重要性难以想象

如何才能将流控规则持久化？

问题1： 持久化在哪？
只要是个数据存储容器即可 redis,mysql ,文件等。 兼容性更好的肯定是nacos.
可以将流控规则就想象成一个配置文件。 nacos作为配置中心存储配置文件

问题2:  具体如何操作
https://spring-cloud-alibaba-group.github.io/github-pages/hoxton/en-us/index.html#_dynamic_data_source_support
对应官方文档，动态数据源支持

主要还是那几步：引pom ,改yml ,启动类

1： 引入sentinel 数据源pom
<dependency>
    <groupId>com.alibaba.csp</groupId>
    <artifactId>sentinel-datasource-nacos</artifactId>
</dependency>

2: 改yml 声明 sentinel的数据源
spring.cloud.sentinel.datasource.ds2.nacos.server-addr=localhost:8848
spring.cloud.sentinel.datasource.ds2.nacos.data-id=sentinel
spring.cloud.sentinel.datasource.ds2.nacos.group-id=DEFAULT_GROUP
spring.cloud.sentinel.datasource.ds2.nacos.data-type=json
spring.cloud.sentinel.datasource.ds2.nacos.rule-type=degrade

spring:
  application:
    name: nacos-payment-provider
  cloud:
    nacos:
      discovery:
        server-addr: localhost:8848 #配置Nacos地址
      transport:
        port: 8719
        dashboard: localhost:8080
    #  datasource:====================
      datasource:
         ds1:
           nacos:
             server-addr: localhost:8848
             dataId: nacos-payment-provider
             groupId: DEFAULT_GROUP
             data-type: json
             rule-type: flow

3:nacos 控制台中需要先配置对应的存储文件，用于存储流控规则

4：启动服务，配置流控规则。 关闭服务，规则消失，但是重启后之前配置仍然存在就标识成功

5： 这样有个缺陷就是每次新增规则都要我们自己将流控规则的配置写到配置文件中去
[
    {
         "resource": "/retaLimit/byUrl",
         "limitApp": "default",
         "grade":   1,
         "count":   1,
         "strategy": 0,
         "controlBehavior": 0,
         "clusterMode": false
    }
]

思考： 有没有一种方式，我们在sentinel中一配置，就自动更新到nacos中去
https://blog.csdn.net/LSY_CSDN_/article/details/105114573
https://www.imooc.com/article/289464 (重点参考)

核心思路：《Alibaba Sentinel 控制台》提供DynamicRulePublisher 和DynamicRuleProvider接口用于实现应用维度的规则推送和拉取
Sentinel 提供应用维度规则推送的示例页面（/v2/flow），用户改造控制台对接配置中心后可直接通过 v2 页面推送规则至配置中心。
说人话就是： 只要通过修改和实现sentinel控制台的DynamicRulePublisher 和DynamicRuleProvider接口 可以实现将规则推送给nacos

对于流控规则、降级规则、系统规则、授权规则、热点规则等5种数据规则，后台修改都是一个思路。
1:数据传输对应的实体类 (官方有提供package com.alibaba.csp.sentinel.dashboard.datasource.entity.rule;)
2:实现DynamicRulePublisher 和DynamicRuleProvider接口，并指定接口传递对应的5种数据规则的实体的集合
DynamicRuleProvider：从Nacos上读取配置
DynamicRulePublisher：将规则推送到Nacos上
(test/java/com/alibaba/csp/sentinel/dashboard/rule/nacos目录 下也有案例)
这里需要注意：DynamicRulePublisher 和DynamicRuleProvider接口用于实现《应用维度》的规则推送和拉取

@Component("flowRuleNacosPublisher")
public class FlowRuleNacosPublisher implements DynamicRulePublisher<List<FlowRuleEntity>> {
    @Autowired
    private ConfigService configService;

    //这里DynamicRulePublisher 和DynamicRuleProvider接口用于实现《应用维度》的规则推送和拉取
    //所以这里需要参数app
    @Override
    public void publish(String app, List<FlowRuleEntity> rules) throws Exception {
        NacosConfigUtil.setRuleStringToNacos(
            this.configService,
            app,
            NacosConfigUtil.FLOW_DATA_ID_POSTFIX,
            rules
        );
    }
}

3:分别重写对应的push、getRules方法，方法核心就是将数据推送到nacos中或从nacos中获取数据


在sentinel控制台源码中向nacos推送数据需要注意：
1:sentinel控制台读取的数据规则格式与sentinel在微服务中的客户端获取的数据规则格式不同
因此在nacos中一种数据规则存储2个dataId的配置文件，
一个是让sentinel控制台读取的，一个是让sentinel微服务中客户端读取的。两个配置文件中配置的数据规则是一致的。

2:sentinel微服务中的客户端会根据配置文件中配置的nacos中的数据文件进行获取并监听配置是否发生变化，
如果发生变化就会自动获取最新数据规则。

https://github.com/eacdy/Sentinel-Dashboard-Nacos/releases/tag/v1.6.2-NACOS

尝试下载下来，看自己能不能复现。

使用上面下载的dashboard 确实可以实现自动通知存储。

重点在于改造dashboard:
思考： dashboard 改造的重点和原理是什么？

在大神的基础上进行优化；
https://blog.csdn.net/LSY_CSDN_/article/details/105114573
1：簇点链路中加的 ，直接加到流控——nacos 中， 目前是直接加到流控内存中


==========================================================
SpringCloud Alibaba Seata处理分布式事务

一次业务操作需要跨多个数据源或需要跨多个系统进行远程调用，传统的事务就无法保证数据的一致性了
每个服务内部的数据一致性由本地事务保证，但是全局的数据一致就无法保证
就会产生分布式事务问题

SpringCloud Alibaba 就提出了Seata 来解决这个问题

整体机制: 都是二阶段提交


Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式
AT 模式  (默认是AT 模式):
一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。
二阶段：
    提交异步化，非常快速地完成。
    回滚通过一阶段的回滚日志进行反向补偿。
(
第一阶段就是 分布式事务中，各个事务去执行自己的本地事务
就是针对要修改的数据有两个锁，一个是全局事务的锁(标志——用来判断全局事务是否回滚) ，一个是本地的锁。
每个本地事务都先执行，然后虚拟提交。并在数据库中记录下业务数据，和回滚日志

第二阶段： 就是统计每个本地事务的情况：
如果每个本地事务都成功，就统一提交
如果有一个子分支事务没有提交就通知各个分支根据自己本地的回滚日志回滚。
)

写隔离：Seata 多个分支操作同一个表是如何避免 “脏写” 的。 避免并发修改提交。

事务在针对一个表进行操作的时候 会先获取本地锁(避免 “并发”)
然后是一个分布式事务有一个全局锁，即当前分布式事务中的分支同一个时刻针对某一个表只能由一个分支来操作(提交更新)。
即在分布式事务中，只有同时获取到本地锁和全局锁，才能对某本表的修改进行提交，入股取锁超时，本地事务回滚。
==========================================
以一个示例来说明：两个全局事务 tx1 和 tx2，分别对 a 表的 m 字段进行更新操作，m 的初始值 1000。
1：tx1 先开始，开启本地事务，拿到本地锁，更新操作 m = 1000 - 100 = 900。
2：tx1 本地事务提交前，先拿到该记录的 全局锁 ，本地提交释放本地锁。
3: tx2 后开始，开启本地事务，拿到本地锁，更新操作 m = 900 - 100 = 800。
4: tx2 本地事务提交前，尝试拿该记录的 全局锁
5: tx1 全局提交前，该记录的全局锁被 tx1 持有，tx2 需要重试等待 全局锁 。
6: tx1 二阶段全局提交成功 则释放 全局锁 。tx2 拿到 全局锁 提交本地事务。
---
7: tx1 的二阶段提交失败，则全局回滚，则 tx1 需要重新获取该数据的本地锁，进行反向补偿的更新操作，实现分支的回滚。
8：tx2 仍在等待该数据的 全局锁 去提交事务，同时持有本地锁。
9：tx1 无法获取本地锁，分支回滚会失败。分支的回滚会一直重试
10：tx2 的 全局锁 等锁超时，放弃 全局锁 并回滚本地事务释放本地锁
11：tx1 的分支回滚最终成功。
因为整个过程 全局锁 在 tx1 结束前一直是被 tx1 持有的，所以不会发生 脏写 的问题
===========================================

读隔离: 如何避免脏读？

在数据库本地事务隔离级别 读已提交（Read Committed） 或以上的基础上，
Seata（AT 模式）出于总体性能上的考虑，的默认全局隔离级别是 读未提交（Read Uncommitted） 容易导致脏读，什么都不能保证。
虽然有并发读，但是本地读安全。 多个读要求并没有写 安全性那么高
mysql数默认的事务处理级别就是【REPEATABLE-READ】，也就是可重复读

一阶段 工作机制：========================================================================
1：分支事务执行sql 解析 SQL：得到 SQL 的类型（UPDATE），表（product），条件（where name = 'TXC'）等相关的信息
2: 查询处执行sql 前，当条记录的镜像值。select id, name, since from product where name = 'TXC';
3：执行业务 SQL：更新这条记录的 name 为 'GTS'。
4：查询后镜像 (执行了业务sql后当前记录的镜像)：根据前镜像的结果，通过 主键 定位数据。
   select id, name, since from product where id = 1;
5：插入回滚日志：把前后镜像数据以及业务 SQL 相关的信息组成一条回滚日志记录，插入到 UNDO_LOG 表中。
6：提交前，向 TC 注册分支：申请 product 表中，主键值等于 1 的记录的 全局锁 。
7：本地事务提交：业务数据的更新和前面步骤中生成的 UNDO LOG 一并提交。
8：将本地事务提交的结果上报给 TC。

二阶段工作机制：========================================================================

二阶段提交================
1：收到 TC 的分支提交请求，把请求放入一个异步任务的队列中，马上返回提交成功的结果给 TC
2：异步任务阶段的分支提交请求将异步和批量地删除相应 UNDO LOG 记录。

二阶段回滚================
收到 TC 的分支回滚请求，开启一个本地事务，执行如下操作。
1：通过 全局事务id XID 和 分支ID  Branch ID 查找到相应的 UNDO LOG 记录。
2：数据校验：拿 UNDO LOG 中的后镜与当前数据进行比较， (这就是生成后镜像的作用)
          如果有不同，说明数据被当前全局事务之外的动作做了修改。这种情况，需要根据配置策略来做处理

3：根据 UNDO LOG 中的前镜像和业务 SQL 的相关信息生成并执行回滚的语句：
4：提交本地事务。并把本地事务的执行结果（即分支事务回滚的结果）上报给 TC。

二阶段工作机制：========================================================================

AT 模式（基础）基于 支持本地 ACID 事务 的 关系型数据库
相应的，TCC 模式，不依赖于底层数据资源的本地事务支持。 是指支持把 自定义 的分支事务纳入到全局事务的管理中。
就是需要自定义回滚和提交逻辑。
Saga模式是SEATA提供的长事务解决方案。
长事务：在Saga模式中，业务流程中每个参与者都直接提交本地事务，当出现某一个参与者失败则补偿前面已经成功的参与者。
      一阶段正向服务和二阶段补偿服务都由业务开发实现
适用场景：业务流程长、业务流程多。参与者包含其它公司或遗留系统服务，无法提供 TCC 模式要求的三个接口
优势：一阶段提交本地事务，无锁，高性能。
     事件驱动架构，参与者可异步执行，高吞吐
     补偿服务易于实现
缺点：
    不保证隔离性



原理是什么？ 1+3
1： 每个分布式事务都有一个全局分布式事务ID； 唯一标识。
    在分布式系统中存在多个子分支(子数据库)， 只要我们具有同一个XID,我么就属于同一个分布式事务

2: XID 会在微服务调用链路的上下文中传播



三组件：

TM： TM (Transaction Manager) - 事务管理器
    定义全局事务的范围：开始全局事务、提交或回滚全局事务。
    统一全局：行军的统帅，教室的班主任。 创建和开启分步事务，并生成全局事务id .
    TM 告诉TC 开启了一个分布式事务，并将XID 传给他，让TC去号召学生。
    (标记了@GlobalTransactional 注解的分布式事务发起方)

TC: TC (Transaction Coordinator) - 事务协调者
    维护全局和分支事务的状态，驱动全局事务提交或回滚。
    顾名思义：协调者。 行军时的通信兵，负责统计各个分支的情况，并报告给统帅。 班级中的班长。 老师与学生沟通的桥梁
    TM 只总领大觉，具体细则的事务还是班长负责，负责收作业
    TC 负责传递TM 的决议。TM 向TC 发起针对XID的提交与回滚。TC 则调度XID下的分支事务RM完成提交和回滚
    (他就是我们的seata 服务器)

RM: RM (Resource Manager) - 资源管理器
    管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚
    注册分布式事务分支，每一个分支对应一个RM (可以简要理解为数据库)
    RM 学生, 负责将作业交给班长TC,所以RM 是向TC 注册分支事务。
    (每个RM  数据库都要有自己的undo 日志表，用于本数据库的回滚日志记录——事务的参与方)

如何使用？
两个注解：Spring 本地@Transactional
        全局@GlobalTransactional (只需要在是业务方法上加这个注解即可)

如何实际来验证？ 准备两个微服务， 分别操作不同数据库，来模拟分布式数据库。

1: 需要准备好TC (seata server )
下载seata server  (新版的配置文件大部分存放在GitHub上 )

https://blog.csdn.net/weixin_43831049/article/details/117446071
配置seata server : 主要修改：自定义事务组名称+事务日志存储模式为db+数据库连接信息
配置三点： 直接更改file.conf.example
1：seata server 日志的默认存储方式是文件，我们需要需要改为存储到mysql 数据库
         file.conf ->store模块： mode = "db" ，然后配置数据库链接
         建立对应数据库 https://github.com/seata/seata/blob/develop/script/server/db/mysql.sql

2：seata server 默认注册也是文件，我们需要将其注册到nacos 中
             registry.conf
3：seata 最主要的配置是 分布式事务的分组名 -》自定义事务组名称
         file.conf ->service模块：vgroup_mapping.my_test_tx_group = "fsp_tx_group"
         (新版1.4.2 本地没找到配置service 模块 ? 猜测：新版本是否已经不需要配置了？ 先跳过)
         没有也可以启动。 不知道后续是否有影响==

         service {
           #transaction service group mapping
           vgroupMapping.my_test_tx_group = "default"
           #only support when registry.type=file, please don't set multiple addresses
           default.grouplist = "127.0.0.1:8091"
           #degrade, current not support
           enableDegrade = false
           #disable seata
           disableGlobalTransaction = false
         }

第一步： 先启动seata  service

第二部： 准备业务数据库 和业务服务

order 调用payment ,各自操作一个数据库，操作一张表

先从payment 开始 ，除了业务表以为，还要建立一个undo_log表
1：建立数据库
2；单个服务连接和操作数据库
============================================================
springboot 整合mybatis
1: 引入操作数据库的pom
        <dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-jdbc</artifactId>
		</dependency>

		<dependency>
			<groupId>org.mybatis.spring.boot</groupId>
			<artifactId>mybatis-spring-boot-starter</artifactId>
			<version>1.3.2</version>
		</dependency>

		<dependency>
			<groupId>mysql</groupId>
			<artifactId>mysql-connector-java</artifactId>
			<scope>runtime</scope>
		</dependency>



2: 配置文件中配置数据源

#配置连接数据库的数据源
spring:
  datasource:
    driver-class-name: com.mysql.jdbc.Driver
    url: jdbc:mysql://localhost:3306/springboot?useUnicode=true&characterEncoding=utf-8&useSSL=true&serverTimezone=UTC
    username: root
    password: 1234

#mybatis映射配置 配置sql文件地址
mybatis:
  mapper-locations: classpath:mapping/*.xml
  type-aliases-package: com.atiguigu.springCloud.entities

#showSql 配置sql 日志输出
logging:
  level:
    com:
      atiguigu:
        springCloud:
                dao : debug



3：写业务类
UserMapper 书写： @Mapper 注解
sql文件书写

service 调用

这里使用IDEA 自动生产

============================================================
3：在微服务上配置和使用seata.

1:pom 中引入
       <!--seata-->
        <dependency>
            <groupId>com.alibaba.cloud</groupId>
            <artifactId>spring-cloud-starter-alibaba-seata</artifactId>
            <exclusions>
                <exclusion>
                    <artifactId>seata-all</artifactId>
                    <groupId>io.seata</groupId>
                </exclusion>
            </exclusions>
        </dependency>
        <!-- 这里要引入和seata  server 对应的版本 -->
         <dependency>
                    <groupId>io.seata</groupId>
                    <artifactId>seata-all</artifactId>
                    <version>1.4.2</version>
         </dependency>
2:配置文件
yml:
================================================
spring:
  application:
    name: seata-order-service
  cloud:
    alibaba:
      seata:
        #自定义事务组名称需要与seata-server中的对应
        tx-service-group: fsp_tx_group
================================================
将seata的配置文件拷贝进resource， 作为seata clinet 的配置文件
file.conf
registry.conf



3: 业务代码使用注解

4：启动seata server 再启动微服务


=======================================
问题： 使用新版的seata server  压根起不起

要不替换成0.9，再按照教程试一遍

注意几点：
1：配置
spring:
  application:
    name: nacos-order-consumer
    #    猜想即使是消费者也需要先注册到那nacos 注册中心处
  cloud:
      nacos:
        discovery:
          server-addr: localhost:8848 #配置Nacos地址
      alibaba:
        seata:
          #自定义事务组名称需要与seata-server中的对应
          tx-service-group: zhaoby_tx_group

只有这个配置对了才能正常启动开启分组

2： 加了注解还是发现不能正常分布式
所有sql都走DataSourceProxy代理；而DataSourceProxy代理是直接commit代码；




















